{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DflwvgE1-cux"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ybressler/Web-Scraping/blob/master/Web_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DflwvgE1-cux",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraper Tool\n",
        "\"Turn on\" the code by executing the first cell of this notebook. Simply hit \"command + enter\" to run it. Then, hide the code by foldin the header section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiyY5iWmlX-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXECUTE THIS CELL\n",
        "\n",
        "# Step 1 \n",
        "# ------------------------------\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Step 2\n",
        "# ------------------------------\n",
        "import re\n",
        "\n",
        "skip_words = [\n",
        "  'like', 'you', 'you\\'ve', 'never',\n",
        "  'before', 'in', 'a', 'an', 'it\\'s', \n",
        "  'it', 'and', 'at', 'the', 'as', \n",
        "  'those', 'off', 'their','is', 'to', \n",
        "  'before','out', 'of', 'for', 'can',\n",
        "  'us', 'our','it\\'s','not','without',\n",
        "  'up','now', 'on','between','happens',\n",
        "  'where','find', 'find',\n",
        "  'while','from','are','then','what','with','being',\n",
        "  'it\\'s','hey','how', 'i','which','about',\n",
        "  'me','why','be','this','has','within','become','my',\n",
        "  'work','one','come','fan','would','perhaps','both',\n",
        "  'its','when','or','no','pm','am',\n",
        "   'here','go','will','learn','buy',\n",
        "    'more','please','sale','he','be','word','by','ftd',\n",
        "    'the', '','let', 'do',\n",
        "\n",
        "]\n",
        "\n",
        "# Get rid of punctuation\n",
        "f_only_words = lambda x: re.sub(r'[^\\w\\s]','', x)\n",
        "\n",
        "def clean_your_tags(words_list):\n",
        "  \"\"\"\n",
        "  cleans it all up!\n",
        "  \"\"\"\n",
        "\n",
        "  if type(words_list)==str:\n",
        "    words_list = f_only_words(words_list).lower()\n",
        "    words_out = [x for x in words_list.split() if x not in skip_words]\n",
        "\n",
        "  else:\n",
        "    words_out = []\n",
        "    for sub_words_list in words_list:\n",
        "      sub_words_list = f_only_words(sub_words_list).lower()\n",
        "      words_out += [x for x in sub_words_list.split() if x not in skip_words]\n",
        "\n",
        "  return words_out\n",
        "\n",
        "# Step 3\n",
        "# ------------------------------\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import inflect\n",
        "\n",
        "def digest_a_web_page( url='https://www.trufflesmystery.com', n_min=1, specific_words = []):\n",
        "  \"\"\"\n",
        "  scrapes a web page and returns a structured dataframe for all the words in the page\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # url = 'https://www.trufflesmystery.com'\n",
        "  r = requests.get(url)\n",
        "  html = r.text\n",
        "\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  # Get the title\n",
        "  title = f_only_words(soup.title.text.lower())\n",
        "\n",
        "  # Get the description\n",
        "  description = soup.find('meta', attrs={'property':'og:description'})\n",
        " \n",
        "  # If description meta tag was found, then get the content\n",
        "  description = f_only_words(description.get('content')) if description else '' \n",
        "\n",
        "  # Get the h tags\n",
        "  h1 = [f_only_words(x.text.lower()) for x in soup.find_all('h1')]\n",
        "  h2 = [f_only_words(x.text.lower()) for x in soup.find_all('h2')]\n",
        "  h3 = [f_only_words(x.text.lower()) for x in soup.find_all('h3')]\n",
        "  body = [f_only_words(x.text.lower()) for x in soup.find_all('p')]\n",
        "\n",
        "  # If that failed...\n",
        "  if len(body)==0:\n",
        "    body = [f_only_words(x.text.lower()) for x in soup.find_all('div', {'class':'paragraph'})]\n",
        "\n",
        "  # get all the words which are left\n",
        "  all_words = []\n",
        "  for x in [title, description, h1, h2, h3, body]:\n",
        "    all_words += clean_your_tags(x)\n",
        "\n",
        "  all_words = [x for x in all_words if len(x)>1]\n",
        "  \n",
        "  # get words with highest counts\n",
        "  unique_words = pd.Series(all_words).value_counts()\n",
        "  unique_words = list(unique_words[unique_words>n_min].keys().values)\n",
        "\n",
        "  # Get all the unique words\n",
        "  unique_words = list(set(unique_words + title.split() + specific_words))\n",
        "\n",
        "\n",
        "  # ------------------------------------------------\n",
        "\n",
        "  records = []\n",
        "  for word in unique_words:\n",
        "    rec = {'word':word}\n",
        "    for x, x_label in zip([title, description, h1, h2, h3, body], ['title', 'description', 'h1', 'h2', 'h3', 'body']):\n",
        "      count = ' '.join(x).count(word) if type(x)!=str else x.count(word)\n",
        "      rec[x_label] = count\n",
        "      \n",
        "    # append your values\n",
        "    records.append(rec)\n",
        "\n",
        "\n",
        "  # All done\n",
        "  df = pd.DataFrame.from_records(records)\n",
        "  \n",
        "  # drop all the skip words\n",
        "  df = df[~df['word'].isin(skip_words)]\n",
        "\n",
        "  # also, merge words that are plural\n",
        "  p = inflect.engine()\n",
        "  p_f = lambda x: p.singular_noun(x) if p.singular_noun(x) else x\n",
        "  df['word'] = df['word'].apply(p_f)\n",
        "  df = df.groupby('word',as_index=False).sum()\n",
        "\n",
        "  df['total (with body)'] = df.sum(axis=1)\n",
        "  df['total (without body)'] = df.drop(columns='total (with body)').sum(axis=1) - df['body']\n",
        "  df.sort_values(by=['total (with body)','total (without body)', 'h1'], ascending=False, inplace=True)\n",
        "  df.reset_index(drop=True,inplace=True)\n",
        "\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "# Step 4\n",
        "# -------------------------------------------------\n",
        "pd.options.display.max_colwidth=100\n",
        "\n",
        "# You want to be able to see\n",
        "pd.set_option('display.max_rows', 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5N1CogDxIac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # -------   To use this tool   ------------\n",
        "# # -------  (for python users)  ------------\n",
        "\n",
        "# # Enter the url you're interested in here:\n",
        "# url = 'https://www.lindonsupply.com/'\n",
        "\n",
        "# # what's the cutoff threshold?\n",
        "# n_min = \n",
        "\n",
        "# # Have any specific words?\n",
        "# specific_words = []\n",
        "\n",
        "# # Do you want to save your file?\n",
        "# save_file = False\n",
        "\n",
        "# # Perform the execution\n",
        "# df = digest_a_web_page(url, n_min=n_min, specific_words=specific_words)\n",
        "\n",
        "# if save_file==True:\n",
        "#   url_name = url.split('.',)[0].split('/')[-1].title()\n",
        "#   file_name = f'Web Scraping â€“ {url_name}.csv'\n",
        "#   with open(file_name, 'w') as f:\n",
        "#     f.write(df.to_csv())\n",
        "#   files.download(file_name)\n",
        "\n",
        "# # Put this here to view\n",
        "# df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCD8r-M22cNS",
        "colab_type": "text"
      },
      "source": [
        "# Try it with a form!\n",
        "\n",
        "Fill out the info and hit \"command + enter\" to see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJi1u8Dp2du4",
        "colab_type": "code",
        "outputId": "b0f6b206-e499-4c8a-ca7b-9de49be64e0c",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#@title Input for Web Scraper\n",
        "#@markdown Words about the stuff. _Okay?_\n",
        "\n",
        "#@markdown For multiple websites, separate each with a comma\n",
        "website = \"http://indicato-famiglie.blogspot.com/2015/12/catalogo-prodotti-coop-art-frankenstein.html\" #@param {type:\"string\"}\n",
        "minimum_cutoff = 0  #@param {type: \"slider\", min: 0, max: 5}\n",
        "\n",
        "#@markdown Enter each word separated by a comma\n",
        "specific_words = 'the, artist, art, coop, co-op' #@param {type: \"string\"}\n",
        "\n",
        "save_file_ = \"No\" #@param [\"Yes\", \"No\"] {allow-input: false}\n",
        "filename = '' #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "specific_words_input = specific_words\n",
        "if ',' in specific_words:\n",
        "  specific_words= [x.strip() for x in specific_words.split(',')]\n",
        "else:\n",
        "  specific_words = [specific_words]\n",
        "  \n",
        "# ----------------------------------------\n",
        "\n",
        "if ',' in website:\n",
        "  # do this\n",
        "  url_list = [x.strip() for x in website.split(',')]\n",
        "  \n",
        "  df_list = []\n",
        "  for url in url_list:\n",
        "    df = digest_a_web_page(url, n_min=minimum_cutoff, specific_words=specific_words)\n",
        "    df_list.append(df)\n",
        "\n",
        "  # merge your dataframes\n",
        "  df = pd.concat(df_list)\n",
        "  df_agg = df.groupby('word', as_index=False).agg(sum)\n",
        "\n",
        "  df_agg.drop(columns='total (with body)', inplace=True)\n",
        "  df_agg.rename(columns = {'total (without body)':'total'},inplace=True)\n",
        "  df_agg.sort_values('total', ascending=False, inplace=True)\n",
        "\n",
        "  # drop small words\n",
        "  df_agg = df_agg[df_agg['word'].apply(len) >=3]\n",
        "\n",
        "  # drop low frequency\n",
        "  df_agg = df_agg[((df_agg['title']>0)|(df_agg['h1']>0)|(df_agg['h2']>=minimum_cutoff))&(df_agg['body']>0)]\n",
        "  df_agg.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # overwrite the previous filename\n",
        "  df = df_agg\n",
        "\n",
        "# ----------------------------------------\n",
        "# Perform the execution\n",
        "\n",
        "else:\n",
        "  url = website\n",
        "df = digest_a_web_page(url, n_min=minimum_cutoff, specific_words=specific_words)\n",
        "\n",
        "# Filter\n",
        "if ',' in specific_words_input:\n",
        "  df = df[df.word.isin(specific_words)].reset_index(drop=True)\n",
        "\n",
        "if save_file_==\"Yes\":\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(df.to_csv())\n",
        "  files.download(filename)\n",
        "else:\n",
        "  # print(df)\n",
        "  None\n",
        "\n",
        "df.head(100)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>h1</th>\n",
              "      <th>h2</th>\n",
              "      <th>h3</th>\n",
              "      <th>body</th>\n",
              "      <th>total (with body)</th>\n",
              "      <th>total (without body)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>art</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>coop</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>artist</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>co-op</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     word  title  description  ...  body  total (with body)  total (without body)\n",
              "0     art      2            0  ...    15                 19                     4\n",
              "1    coop      1            0  ...    14                 16                     2\n",
              "2  artist      0            0  ...     5                  5                     0\n",
              "3   co-op      0            0  ...     0                  0                     0\n",
              "\n",
              "[4 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hxVXsmi8o00",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}